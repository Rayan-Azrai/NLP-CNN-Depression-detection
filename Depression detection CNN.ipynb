{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958342ed",
   "metadata": {},
   "source": [
    "# Import Libraries and Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6eb4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4fcba2",
   "metadata": {},
   "source": [
    "# Check if TensorFlow detects a GPU \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d91f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs detected:\", gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb898d7",
   "metadata": {},
   "source": [
    "# Load and Inspect the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82d75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV file\n",
    "data_path = r\"C:\\Users\\rayon\\Jam3a AI\\Graduation project\\(FINAL)merged_dataset.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows to check the format\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fd4a8",
   "metadata": {},
   "source": [
    "# Preprocess the Data and Split into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c3a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map textual labels to integers\n",
    "label_mapping = {\"depressed\": 1, \"non_depressed\": 0}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "# Print the distribution of labels\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Split the dataset (80% training, 20% validation)\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(\"Number of training samples:\", len(train_df))\n",
    "print(\"Number of validation samples:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903d734",
   "metadata": {},
   "source": [
    "# Create a Text Vectorization Layer and Prepare tf.data Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d54267",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817e346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10000          \n",
    "sequence_length = 200       \n",
    "embedding_dim = 128         \n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e6487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextVectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the vectorization layer to the training text data\n",
    "vectorize_layer.adapt(train_df['text'].values)\n",
    "\n",
    "# Create TensorFlow datasets from the Pandas DataFrame\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['label'].values))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_df['text'].values, val_df['label'].values))\n",
    "\n",
    "# Filter out any empty strings (if any slipped through)\n",
    "def non_empty_text(text, label):\n",
    "    return tf.strings.length(tf.strings.strip(text)) > 0\n",
    "\n",
    "train_ds = train_ds.filter(non_empty_text)\n",
    "val_ds = val_ds.filter(non_empty_text)\n",
    "\n",
    "# Map the datasets to vectorize the text, then batch and prefetch\n",
    "train_ds = train_ds.map(lambda x, y: (vectorize_layer(x), y)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: (vectorize_layer(x), y)).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02acc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorize_layer.get_vocabulary())  # Actual size learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950de28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.get_vocabulary()[:10]  # Most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6415a79",
   "metadata": {},
   "source": [
    "#  Build the CNN Model for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "346131cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    # Embedding layer: converts integer sequences to dense vectors\n",
    "    layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim, input_length=sequence_length),\n",
    "\n",
    "    # Convolutional layer for feature extraction\n",
    "    #          filters,kernel size \n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    \n",
    "    # Global max pooling to reduce each feature map to a single value\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "            \n",
    "    # Fully connected layer with dropout for regularization\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # Output layer: Sigmoid activation for binary classification\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with an appropriate loss and optimizer\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb46d19",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5b429",
   "metadata": {},
   "source": [
    "## Early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0310059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the EarlyStopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bb41d",
   "metadata": {},
   "source": [
    "# Evaluate and Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve the training history\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Use the actual number of epochs that ran for the x-axis\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f8861",
   "metadata": {},
   "source": [
    "# Evaluate model performance on the validation set using additional metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f91f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# Get predicted probabilities on the validation dataset\n",
    "y_pred_prob = model.predict(val_ds)\n",
    "\n",
    "# Convert predicted probabilities to binary predictions (threshold: 0.5)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Retrieve true labels from the validation dataset\n",
    "# We concatenate all the label batches into one numpy array\n",
    "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Validation Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Validation Precision: {:.4f}\".format(precision))\n",
    "print(\"Validation Recall: {:.4f}\".format(recall))\n",
    "print(\"Validation F1 Score: {:.4f}\".format(f1))\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df7255",
   "metadata": {},
   "source": [
    "## Barchart graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the computed metrics\n",
    "metrics_dict = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1\n",
    "}\n",
    "\n",
    "# Plot the metrics as a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(metrics_dict.keys(), metrics_dict.values(), color=[\"blue\", \"green\", \"orange\", \"purple\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Metrics\")\n",
    "\n",
    "# Annotate each bar with its value\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, height + 0.01, f\"{height:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab6ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_kernel)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
